{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.   Load pretrained model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T13:12:15.263840Z",
     "iopub.status.busy": "2024-03-12T13:12:15.263609Z",
     "iopub.status.idle": "2024-03-12T13:12:22.942226Z",
     "shell.execute_reply": "2024-03-12T13:12:22.941723Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d544b7c3ae4bb6941deddd0799e946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PromptEncoderConfig, get_peft_model, TaskType\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"internlm/internlm2-chat-1_8b\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"internlm/internlm2-chat-1_8b\", device_map=\"cuda\",trust_remote_code=True, torch_dtype=torch.float16)\n",
    "# model = model.eval()\n",
    "config = PromptEncoderConfig(\n",
    "    peft_type=\"P_TUNING\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    num_virtual_tokens=10,\n",
    "    token_dim=2048,\n",
    "    # num_transformer_submodules=1,\n",
    "    # num_attention_heads=12,\n",
    "    # num_layers=12,\n",
    "    encoder_reparameterization_type=\"MLP\",\n",
    "    encoder_hidden_size=2048,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.   Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T13:12:22.946200Z",
     "iopub.status.busy": "2024-03-12T13:12:22.945853Z",
     "iopub.status.idle": "2024-03-12T13:12:23.296985Z",
     "shell.execute_reply": "2024-03-12T13:12:23.296483Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# DataFrame to Json\n",
    "df_short_ans = pd.read_excel('./data/2024-02-28-公告测评集.xls', header=0)\n",
    "df_short_ans['answer'] = df_short_ans['answer'].astype(str)\n",
    "# df_short_ans.to_json('./data/data_short_ans_train.json',orient='records')\n",
    "\n",
    "df_choice = pd.read_excel('./data/2024-02-28-公告测评集-选项.xls', header=0)\n",
    "df_choice['prompt'] = ''\n",
    "for i, row in df_choice.iterrows():\n",
    "    prompt = '''下面是几条对上述公告相关的理解或问题答案，请选出一条或多条理解到位、答案准确的选项。\\n\\n'''\n",
    "    question = row['评测问题'] + '\\n\\n'\n",
    "    choice = (\n",
    "        \"选项A:\" + str(row['选项A']), \"选项B:\" + str(row['选项B']), \"选项C:\" + str(row['选项C']), \"选项D:\" + str(row['选项D'])\n",
    "    )\n",
    "    choice_str = \"\\n\".join(choice)\n",
    "    df_choice.loc[i, 'question'] = question + prompt + choice_str\n",
    "\n",
    "df_choice_2 = df_choice[['领域分类', 'question', '答案']].copy()\n",
    "df_choice_2.rename(columns={'领域分类': 'type', '答案': 'answer'}, inplace=True)\n",
    "# df_choice_2.to_json('./data/data_choice_train.json',orient='records')\n",
    "\n",
    "df_combine = pd.concat([df_short_ans, df_choice_2], axis=0, ignore_index=True)\n",
    "df_combine.to_json('./data/data_combine_train.json',orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.   Load dataset and encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T13:12:23.299612Z",
     "iopub.status.busy": "2024-03-12T13:12:23.299362Z",
     "iopub.status.idle": "2024-03-12T13:12:24.930442Z",
     "shell.execute_reply": "2024-03-12T13:12:24.929880Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5008f08af93b4f4cad6418fb157552e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32eaf640e11e49f48d5ceb5859f619ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq, DataCollatorForLanguageModeling\n",
    "from typing import Dict\n",
    "from datasets import Dataset, load_dataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def encode_fn(text, tokenizer, max_length,return_attention_mask=False):\n",
    "    return tokenizer(text, max_length=max_length, padding=\"max_length\", truncation=True,return_attention_mask=return_attention_mask)\n",
    "\n",
    "\n",
    "def get_dataset(file: str, split: str, encode_fn: callable, encode_args: dict,  cache_dir: str='.cache') -> Dataset:\n",
    "    \"\"\"\n",
    "    Load a dataset\n",
    "    \"\"\"\n",
    "    eos_token = tokenizer.eos_token\n",
    "    dataset = load_dataset('json', data_files=file, split=split, cache_dir=cache_dir)\n",
    "    def merge_prompt_and_responses(sample: dict):\n",
    "        # add an eos token note that end of sentence, using in generate.\n",
    "        # encoded_prompt = tokenizer([e + eos_token for e in sample['question']], truncation=False, padding=True, return_attention_mask=True)\n",
    "        # encoded_response = tokenizer([e + eos_token for e in sample['answer']], truncation=False, padding=True, return_attention_mask=False)\n",
    "        encoded_prompt = tokenizer(sample['question'] + eos_token, truncation=False, padding=True, return_attention_mask=True)\n",
    "        encoded_response = tokenizer(sample['answer'] + eos_token, truncation=False, padding=True, return_attention_mask=False)\n",
    "        encoded_q_type = tokenizer(sample['type'] + eos_token, truncation=False, padding=True, return_attention_mask=True)\n",
    "        # input_ids = [np.array(item + [eos_token_id], dtype=np.uint32) for item in encoded_prompt[\"input_ids\"]]\n",
    "        # labels = [np.array(item + [eos_token_id], dtype=np.uint32) for item in encoded_response[\"input_ids\"]]\n",
    "        # prompt = encode_fn(sample['question'] + '[EOS]', return_attention_mask=True)\n",
    "        # answer = encode_fn(sample['answer'] + '[EOS]', return_attention_mask=False)\n",
    "        # title = encode_fn(sample['title'] + '[EOS]', **encode_args)\n",
    "        # print(type(encoded_prompt.input_ids),'\\n',type(encoded_prompt.attention_mask),'\\n',labels)\n",
    "        return {\n",
    "            'input_ids': encoded_prompt.input_ids,\n",
    "            'q_type': encoded_q_type.input_ids,\n",
    "            'labels': encoded_response.input_ids,       \n",
    "            'attention_mask': encoded_prompt.attention_mask,\n",
    "            'q_type_attention_mask' : encoded_q_type.attention_mask,\n",
    "        }\n",
    "\n",
    "    # dataset = dataset.map(merge_prompt_and_responses, batched=True, batch_size=1)\n",
    "    dataset = dataset.map(merge_prompt_and_responses)\n",
    "    return dataset\n",
    "\n",
    "dataset = get_dataset(\n",
    "    file='./data/data_combine_train.json', \n",
    "    split=\"train\", \n",
    "    encode_fn=encode_fn, \n",
    "    encode_args={\"tokenizer\": tokenizer, \"max_length\": 1024}, \n",
    "    cache_dir=\".cache\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.   Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T13:12:24.932587Z",
     "iopub.status.busy": "2024-03-12T13:12:24.932325Z",
     "iopub.status.idle": "2024-03-12T13:17:18.293596Z",
     "shell.execute_reply": "2024-03-12T13:17:18.292553Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: answer, question, q_type_attention_mask, type, q_type. If answer, question, q_type_attention_mask, type, q_type are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Num examples = 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Num Epochs = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Instantaneous batch size per device = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Total train batch size (w. parallel, distributed & accumulation) = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Gradient Accumulation steps = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Total optimization steps = 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Number of trainable parameters = 12,609,536\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 04:51, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.028300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.882500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.782800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.635800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.608400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.557700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.545900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.523300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.485900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.442600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.398400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.403300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.316700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.362700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.286500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.296300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.284700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.214700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.224000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.212400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./train_result/checkpoint-10 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./train_result/checkpoint-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/snapshots/f842a52ed9579d662d5e67c8c0f8c67b3eb877a8/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config InternLM2Config {\n",
      "  \"architectures\": [\n",
      "    \"InternLM2ForCausalLM\"\n",
      "  ],\n",
      "  \"attn_implementation\": \"eager\",\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"internlm/internlm2-chat-1_8b--configuration_internlm2.InternLM2Config\",\n",
      "    \"AutoModel\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\",\n",
      "    \"AutoModelForCausalLM\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\"\n",
      "  },\n",
      "  \"bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"internlm2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 2.0,\n",
      "    \"type\": \"dynamic\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 92544\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./train_result/checkpoint-20 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./train_result/checkpoint-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/snapshots/f842a52ed9579d662d5e67c8c0f8c67b3eb877a8/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config InternLM2Config {\n",
      "  \"architectures\": [\n",
      "    \"InternLM2ForCausalLM\"\n",
      "  ],\n",
      "  \"attn_implementation\": \"eager\",\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"internlm/internlm2-chat-1_8b--configuration_internlm2.InternLM2Config\",\n",
      "    \"AutoModel\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\",\n",
      "    \"AutoModelForCausalLM\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\"\n",
      "  },\n",
      "  \"bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"internlm2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 2.0,\n",
      "    \"type\": \"dynamic\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 92544\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./train_result/checkpoint-30 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./train_result/checkpoint-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/snapshots/f842a52ed9579d662d5e67c8c0f8c67b3eb877a8/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config InternLM2Config {\n",
      "  \"architectures\": [\n",
      "    \"InternLM2ForCausalLM\"\n",
      "  ],\n",
      "  \"attn_implementation\": \"eager\",\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"internlm/internlm2-chat-1_8b--configuration_internlm2.InternLM2Config\",\n",
      "    \"AutoModel\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\",\n",
      "    \"AutoModelForCausalLM\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\"\n",
      "  },\n",
      "  \"bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"internlm2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 2.0,\n",
      "    \"type\": \"dynamic\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 92544\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./train_result/checkpoint-40 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./train_result/checkpoint-40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/snapshots/f842a52ed9579d662d5e67c8c0f8c67b3eb877a8/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config InternLM2Config {\n",
      "  \"architectures\": [\n",
      "    \"InternLM2ForCausalLM\"\n",
      "  ],\n",
      "  \"attn_implementation\": \"eager\",\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"internlm/internlm2-chat-1_8b--configuration_internlm2.InternLM2Config\",\n",
      "    \"AutoModel\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\",\n",
      "    \"AutoModelForCausalLM\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\"\n",
      "  },\n",
      "  \"bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"internlm2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 2.0,\n",
      "    \"type\": \"dynamic\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 92544\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./train_result/checkpoint-50 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./train_result/checkpoint-50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/snapshots/f842a52ed9579d662d5e67c8c0f8c67b3eb877a8/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config InternLM2Config {\n",
      "  \"architectures\": [\n",
      "    \"InternLM2ForCausalLM\"\n",
      "  ],\n",
      "  \"attn_implementation\": \"eager\",\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"internlm/internlm2-chat-1_8b--configuration_internlm2.InternLM2Config\",\n",
      "    \"AutoModel\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\",\n",
      "    \"AutoModelForCausalLM\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\"\n",
      "  },\n",
      "  \"bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"internlm2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 2.0,\n",
      "    \"type\": \"dynamic\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 92544\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./train_result/checkpoint-60 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./train_result/checkpoint-60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/snapshots/f842a52ed9579d662d5e67c8c0f8c67b3eb877a8/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config InternLM2Config {\n",
      "  \"architectures\": [\n",
      "    \"InternLM2ForCausalLM\"\n",
      "  ],\n",
      "  \"attn_implementation\": \"eager\",\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"internlm/internlm2-chat-1_8b--configuration_internlm2.InternLM2Config\",\n",
      "    \"AutoModel\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\",\n",
      "    \"AutoModelForCausalLM\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\"\n",
      "  },\n",
      "  \"bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"internlm2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 2.0,\n",
      "    \"type\": \"dynamic\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 92544\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./train_result/checkpoint-70 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./train_result/checkpoint-70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/snapshots/f842a52ed9579d662d5e67c8c0f8c67b3eb877a8/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config InternLM2Config {\n",
      "  \"architectures\": [\n",
      "    \"InternLM2ForCausalLM\"\n",
      "  ],\n",
      "  \"attn_implementation\": \"eager\",\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"internlm/internlm2-chat-1_8b--configuration_internlm2.InternLM2Config\",\n",
      "    \"AutoModel\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\",\n",
      "    \"AutoModelForCausalLM\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\"\n",
      "  },\n",
      "  \"bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"internlm2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 2.0,\n",
      "    \"type\": \"dynamic\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 92544\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./train_result/checkpoint-80 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./train_result/checkpoint-80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/snapshots/f842a52ed9579d662d5e67c8c0f8c67b3eb877a8/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config InternLM2Config {\n",
      "  \"architectures\": [\n",
      "    \"InternLM2ForCausalLM\"\n",
      "  ],\n",
      "  \"attn_implementation\": \"eager\",\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"internlm/internlm2-chat-1_8b--configuration_internlm2.InternLM2Config\",\n",
      "    \"AutoModel\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\",\n",
      "    \"AutoModelForCausalLM\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\"\n",
      "  },\n",
      "  \"bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"internlm2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 2.0,\n",
      "    \"type\": \"dynamic\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 92544\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./train_result/checkpoint-90 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./train_result/checkpoint-90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/snapshots/f842a52ed9579d662d5e67c8c0f8c67b3eb877a8/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config InternLM2Config {\n",
      "  \"architectures\": [\n",
      "    \"InternLM2ForCausalLM\"\n",
      "  ],\n",
      "  \"attn_implementation\": \"eager\",\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"internlm/internlm2-chat-1_8b--configuration_internlm2.InternLM2Config\",\n",
      "    \"AutoModel\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\",\n",
      "    \"AutoModelForCausalLM\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\"\n",
      "  },\n",
      "  \"bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"internlm2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 2.0,\n",
      "    \"type\": \"dynamic\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 92544\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./train_result/checkpoint-100 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./train_result/checkpoint-100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/snapshots/f842a52ed9579d662d5e67c8c0f8c67b3eb877a8/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config InternLM2Config {\n",
      "  \"architectures\": [\n",
      "    \"InternLM2ForCausalLM\"\n",
      "  ],\n",
      "  \"attn_implementation\": \"eager\",\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"internlm/internlm2-chat-1_8b--configuration_internlm2.InternLM2Config\",\n",
      "    \"AutoModel\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\",\n",
      "    \"AutoModelForCausalLM\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\"\n",
      "  },\n",
      "  \"bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"internlm2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 2.0,\n",
      "    \"type\": \"dynamic\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 92544\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./train_result/checkpoint-110 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./train_result/checkpoint-110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/snapshots/f842a52ed9579d662d5e67c8c0f8c67b3eb877a8/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config InternLM2Config {\n",
      "  \"architectures\": [\n",
      "    \"InternLM2ForCausalLM\"\n",
      "  ],\n",
      "  \"attn_implementation\": \"eager\",\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"internlm/internlm2-chat-1_8b--configuration_internlm2.InternLM2Config\",\n",
      "    \"AutoModel\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\",\n",
      "    \"AutoModelForCausalLM\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\"\n",
      "  },\n",
      "  \"bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"internlm2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 2.0,\n",
      "    \"type\": \"dynamic\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 92544\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./train_result/checkpoint-120 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./train_result/checkpoint-120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/snapshots/f842a52ed9579d662d5e67c8c0f8c67b3eb877a8/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config InternLM2Config {\n",
      "  \"architectures\": [\n",
      "    \"InternLM2ForCausalLM\"\n",
      "  ],\n",
      "  \"attn_implementation\": \"eager\",\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"internlm/internlm2-chat-1_8b--configuration_internlm2.InternLM2Config\",\n",
      "    \"AutoModel\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\",\n",
      "    \"AutoModelForCausalLM\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\"\n",
      "  },\n",
      "  \"bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"internlm2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 2.0,\n",
      "    \"type\": \"dynamic\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 92544\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./train_result/checkpoint-130 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./train_result/checkpoint-130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/snapshots/f842a52ed9579d662d5e67c8c0f8c67b3eb877a8/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config InternLM2Config {\n",
      "  \"architectures\": [\n",
      "    \"InternLM2ForCausalLM\"\n",
      "  ],\n",
      "  \"attn_implementation\": \"eager\",\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"internlm/internlm2-chat-1_8b--configuration_internlm2.InternLM2Config\",\n",
      "    \"AutoModel\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\",\n",
      "    \"AutoModelForCausalLM\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\"\n",
      "  },\n",
      "  \"bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"internlm2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 2.0,\n",
      "    \"type\": \"dynamic\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 92544\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./train_result/checkpoint-140 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./train_result/checkpoint-140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/snapshots/f842a52ed9579d662d5e67c8c0f8c67b3eb877a8/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config InternLM2Config {\n",
      "  \"architectures\": [\n",
      "    \"InternLM2ForCausalLM\"\n",
      "  ],\n",
      "  \"attn_implementation\": \"eager\",\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"internlm/internlm2-chat-1_8b--configuration_internlm2.InternLM2Config\",\n",
      "    \"AutoModel\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\",\n",
      "    \"AutoModelForCausalLM\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\"\n",
      "  },\n",
      "  \"bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"internlm2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 2.0,\n",
      "    \"type\": \"dynamic\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 92544\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./train_result/checkpoint-150 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./train_result/checkpoint-150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/snapshots/f842a52ed9579d662d5e67c8c0f8c67b3eb877a8/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config InternLM2Config {\n",
      "  \"architectures\": [\n",
      "    \"InternLM2ForCausalLM\"\n",
      "  ],\n",
      "  \"attn_implementation\": \"eager\",\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"internlm/internlm2-chat-1_8b--configuration_internlm2.InternLM2Config\",\n",
      "    \"AutoModel\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\",\n",
      "    \"AutoModelForCausalLM\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\"\n",
      "  },\n",
      "  \"bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"internlm2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 2.0,\n",
      "    \"type\": \"dynamic\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 92544\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./train_result/checkpoint-160 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./train_result/checkpoint-160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/snapshots/f842a52ed9579d662d5e67c8c0f8c67b3eb877a8/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config InternLM2Config {\n",
      "  \"architectures\": [\n",
      "    \"InternLM2ForCausalLM\"\n",
      "  ],\n",
      "  \"attn_implementation\": \"eager\",\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"internlm/internlm2-chat-1_8b--configuration_internlm2.InternLM2Config\",\n",
      "    \"AutoModel\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\",\n",
      "    \"AutoModelForCausalLM\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\"\n",
      "  },\n",
      "  \"bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"internlm2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 2.0,\n",
      "    \"type\": \"dynamic\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 92544\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./train_result/checkpoint-170 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./train_result/checkpoint-170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/snapshots/f842a52ed9579d662d5e67c8c0f8c67b3eb877a8/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config InternLM2Config {\n",
      "  \"architectures\": [\n",
      "    \"InternLM2ForCausalLM\"\n",
      "  ],\n",
      "  \"attn_implementation\": \"eager\",\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"internlm/internlm2-chat-1_8b--configuration_internlm2.InternLM2Config\",\n",
      "    \"AutoModel\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\",\n",
      "    \"AutoModelForCausalLM\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\"\n",
      "  },\n",
      "  \"bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"internlm2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 2.0,\n",
      "    \"type\": \"dynamic\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 92544\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./train_result/checkpoint-180 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./train_result/checkpoint-180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/snapshots/f842a52ed9579d662d5e67c8c0f8c67b3eb877a8/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config InternLM2Config {\n",
      "  \"architectures\": [\n",
      "    \"InternLM2ForCausalLM\"\n",
      "  ],\n",
      "  \"attn_implementation\": \"eager\",\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"internlm/internlm2-chat-1_8b--configuration_internlm2.InternLM2Config\",\n",
      "    \"AutoModel\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\",\n",
      "    \"AutoModelForCausalLM\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\"\n",
      "  },\n",
      "  \"bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"internlm2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 2.0,\n",
      "    \"type\": \"dynamic\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 92544\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./train_result/checkpoint-190 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./train_result/checkpoint-190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/snapshots/f842a52ed9579d662d5e67c8c0f8c67b3eb877a8/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config InternLM2Config {\n",
      "  \"architectures\": [\n",
      "    \"InternLM2ForCausalLM\"\n",
      "  ],\n",
      "  \"attn_implementation\": \"eager\",\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"internlm/internlm2-chat-1_8b--configuration_internlm2.InternLM2Config\",\n",
      "    \"AutoModel\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\",\n",
      "    \"AutoModelForCausalLM\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\"\n",
      "  },\n",
      "  \"bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"internlm2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 2.0,\n",
      "    \"type\": \"dynamic\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 92544\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./train_result/checkpoint-200 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./train_result/checkpoint-200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/snapshots/f842a52ed9579d662d5e67c8c0f8c67b3eb877a8/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config InternLM2Config {\n",
      "  \"architectures\": [\n",
      "    \"InternLM2ForCausalLM\"\n",
      "  ],\n",
      "  \"attn_implementation\": \"eager\",\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"internlm/internlm2-chat-1_8b--configuration_internlm2.InternLM2Config\",\n",
      "    \"AutoModel\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\",\n",
      "    \"AutoModelForCausalLM\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\"\n",
      "  },\n",
      "  \"bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"internlm2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 2.0,\n",
      "    \"type\": \"dynamic\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 92544\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='./train_result/',\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    auto_find_batch_size=True,  # 防止OOM\n",
    "    gradient_accumulation_steps=10,\n",
    "    learning_rate=1e-3,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=10,\n",
    "    log_level='info',\n",
    "    save_steps=10,\n",
    "    save_total_limit=20,\n",
    "    # fp16=config.fp16,\n",
    "    # logging_first_step=config.logging_first_step,\n",
    "    warmup_steps=50,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# trainer\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = args,\n",
    "    train_dataset = dataset,\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm=False)\n",
    "    # data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer)\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "trainer.train()\n",
    "\n",
    "loss_log = pd.DataFrame(trainer.state.log_history)\n",
    "loss_log.to_csv(f\"{'./logs'}/p_tune_train_log_{time.strftime('%Y%m%d-%H%M')}.csv\")\n",
    "\n",
    "# trainer.save_model('./trained_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T13:17:18.296928Z",
     "iopub.status.busy": "2024-03-12T13:17:18.296462Z",
     "iopub.status.idle": "2024-03-12T13:17:18.885693Z",
     "shell.execute_reply": "2024-03-12T13:17:18.884599Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/snapshots/f842a52ed9579d662d5e67c8c0f8c67b3eb877a8/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config InternLM2Config {\n",
      "  \"architectures\": [\n",
      "    \"InternLM2ForCausalLM\"\n",
      "  ],\n",
      "  \"attn_implementation\": \"eager\",\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"internlm/internlm2-chat-1_8b--configuration_internlm2.InternLM2Config\",\n",
      "    \"AutoModel\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\",\n",
      "    \"AutoModelForCausalLM\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\"\n",
      "  },\n",
      "  \"bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"internlm2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 2.0,\n",
      "    \"type\": \"dynamic\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 92544\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained('./trained_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T13:17:18.889065Z",
     "iopub.status.busy": "2024-03-12T13:17:18.888470Z",
     "iopub.status.idle": "2024-03-12T13:17:18.895541Z",
     "shell.execute_reply": "2024-03-12T13:17:18.894464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): InternLM2ForCausalLM(\n",
      "    (model): InternLM2Model(\n",
      "      (tok_embeddings): Embedding(92544, 2048, padding_idx=2)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x InternLM2DecoderLayer(\n",
      "          (attention): InternLM2Attention(\n",
      "            (wqkv): Linear(in_features=2048, out_features=4096, bias=False)\n",
      "            (wo): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (rotary_emb): InternLM2DynamicNTKScalingRotaryEmbedding()\n",
      "          )\n",
      "          (feed_forward): InternLM2MLP(\n",
      "            (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "            (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "            (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (attention_norm): InternLM2RMSNorm()\n",
      "          (ffn_norm): InternLM2RMSNorm()\n",
      "        )\n",
      "      )\n",
      "      (norm): InternLM2RMSNorm()\n",
      "    )\n",
      "    (output): Linear(in_features=2048, out_features=92544, bias=False)\n",
      "  )\n",
      "  (prompt_encoder): ModuleDict(\n",
      "    (default): PromptEncoder(\n",
      "      (embedding): Embedding(10, 2048)\n",
      "      (mlp_head): Sequential(\n",
      "        (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_embeddings): Embedding(92544, 2048, padding_idx=2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.   Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T13:17:18.898787Z",
     "iopub.status.busy": "2024-03-12T13:17:18.898157Z",
     "iopub.status.idle": "2024-03-12T13:17:22.940647Z",
     "shell.execute_reply": "2024-03-12T13:17:22.940057Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/snapshots/f842a52ed9579d662d5e67c8c0f8c67b3eb877a8/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/snapshots/f842a52ed9579d662d5e67c8c0f8c67b3eb877a8/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config InternLM2Config {\n",
      "  \"_name_or_path\": \"internlm/internlm2-chat-1_8b\",\n",
      "  \"architectures\": [\n",
      "    \"InternLM2ForCausalLM\"\n",
      "  ],\n",
      "  \"attn_implementation\": \"eager\",\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"internlm/internlm2-chat-1_8b--configuration_internlm2.InternLM2Config\",\n",
      "    \"AutoModel\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\",\n",
      "    \"AutoModelForCausalLM\": \"internlm/internlm2-chat-1_8b--modeling_internlm2.InternLM2ForCausalLM\"\n",
      "  },\n",
      "  \"bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"internlm2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 2.0,\n",
      "    \"type\": \"dynamic\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 92544\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/snapshots/f842a52ed9579d662d5e67c8c0f8c67b3eb877a8/model.safetensors.index.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Instantiating InternLM2ForCausalLM model under default dtype torch.float16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce9fc1aee02845d9883177f0a8ff426d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing InternLM2ForCausalLM.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All the weights of InternLM2ForCausalLM were initialized from the model checkpoint at internlm/internlm2-chat-1_8b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use InternLM2ForCausalLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/snapshots/f842a52ed9579d662d5e67c8c0f8c67b3eb877a8/generation_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftConfig, PeftModelForCausalLM\n",
    "peft_config = PeftConfig.from_pretrained('./trained_model/')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"internlm/internlm2-chat-1_8b\", device_map=\"cuda\",trust_remote_code=True, torch_dtype=torch.float16)\n",
    "model = PeftModelForCausalLM.from_pretrained(model, './trained_model', config=peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T13:17:22.943143Z",
     "iopub.status.busy": "2024-03-12T13:17:22.942842Z",
     "iopub.status.idle": "2024-03-12T13:17:22.946818Z",
     "shell.execute_reply": "2024-03-12T13:17:22.946357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): InternLM2ForCausalLM(\n",
      "    (model): InternLM2Model(\n",
      "      (tok_embeddings): Embedding(92544, 2048, padding_idx=2)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x InternLM2DecoderLayer(\n",
      "          (attention): InternLM2Attention(\n",
      "            (wqkv): Linear(in_features=2048, out_features=4096, bias=False)\n",
      "            (wo): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (rotary_emb): InternLM2DynamicNTKScalingRotaryEmbedding()\n",
      "          )\n",
      "          (feed_forward): InternLM2MLP(\n",
      "            (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "            (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "            (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (attention_norm): InternLM2RMSNorm()\n",
      "          (ffn_norm): InternLM2RMSNorm()\n",
      "        )\n",
      "      )\n",
      "      (norm): InternLM2RMSNorm()\n",
      "    )\n",
      "    (output): Linear(in_features=2048, out_features=92544, bias=False)\n",
      "  )\n",
      "  (prompt_encoder): ModuleDict(\n",
      "    (default): PromptEncoder(\n",
      "      (embedding): Embedding(10, 2048)\n",
      "    )\n",
      "  )\n",
      "  (word_embeddings): Embedding(92544, 2048, padding_idx=2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "066cbeeb05fc43deb600a86ed0125fda": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "074f0c2f7d5b4b5cac2b15ed751b15b2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0814f3a0b9534d95a4ded6ee9f5b2d83": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b97c3c2a7c9748ef92d71079f8b0430b",
       "placeholder": "​",
       "style": "IPY_MODEL_98452dd6bf8945a68c70c8ac57ffc371",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "187fd4694126416aacea9d6bbc8919d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1d0f4232e1a84b968f95047957a86786",
       "placeholder": "​",
       "style": "IPY_MODEL_41253d55c2704ba095dcbab94ab0a088",
       "tabbable": null,
       "tooltip": null,
       "value": " 200/0 [00:00&lt;00:00, 9882.20 examples/s]"
      }
     },
     "1c98a70f399c4c1aa1344adc68948aac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_074f0c2f7d5b4b5cac2b15ed751b15b2",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c7f175d5d02044ca9e4c81fa8bba6840",
       "tabbable": null,
       "tooltip": null,
       "value": 2.0
      }
     },
     "1cce82aad91d40eca9cba30bb3422488": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1d0f4232e1a84b968f95047957a86786": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1e5af73395424db8bfb4fe5502481e0f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "24be7c7e28f3454e95762c889a575126": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "27bdf701acb346dd89b0e46fb4fed7bb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "32eaf640e11e49f48d5ceb5859f619ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4db9921eb3f848e79dae550c63253488",
        "IPY_MODEL_b723504eb3024d6baabe1d4896d71d08",
        "IPY_MODEL_7e41fd18170d40abbcaaa8a034e26638"
       ],
       "layout": "IPY_MODEL_a077a3ccb1f546449568a1279b43d638",
       "tabbable": null,
       "tooltip": null
      }
     },
     "3382da1816c543e98d405cb7d4c3c1eb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "3b02080e5f144fff83fff08f4d247e1b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3eb444c87ffb42b69fac4b1694a10843": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6cd2f2332ca94f2a86bceedc5c957ce2",
       "placeholder": "​",
       "style": "IPY_MODEL_3b02080e5f144fff83fff08f4d247e1b",
       "tabbable": null,
       "tooltip": null,
       "value": " 1/1 [00:00&lt;00:00, 198.39it/s]"
      }
     },
     "41253d55c2704ba095dcbab94ab0a088": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4261e71c289b41a79b0bf5788b089440": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "42ea922fac7b4269bbce334d88fbe12b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_27bdf701acb346dd89b0e46fb4fed7bb",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b4e53b5db4f34d49a22e4d09d373d326",
       "tabbable": null,
       "tooltip": null,
       "value": 2.0
      }
     },
     "48a45a62e9f144a3b98579d281bf7c43": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4db9921eb3f848e79dae550c63253488": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9c6481d7cfb6423d911b75a128c42e0e",
       "placeholder": "​",
       "style": "IPY_MODEL_6013f82141574ce3a8db1d226403082e",
       "tabbable": null,
       "tooltip": null,
       "value": "Map: 100%"
      }
     },
     "5008f08af93b4f4cad6418fb157552e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b005f0589ff544c2a2d41109d39a9272",
        "IPY_MODEL_982ee5642a634d698866275620db3c89",
        "IPY_MODEL_187fd4694126416aacea9d6bbc8919d8"
       ],
       "layout": "IPY_MODEL_c914986343574503bb5d2a3534ae629c",
       "tabbable": null,
       "tooltip": null
      }
     },
     "5d23118534034678a91c4789d62eefb5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6013f82141574ce3a8db1d226403082e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "63d43ae1b85c4a04b2f7aa28bcd34a1e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "640183fa79864b39ad7d869fc11b9e00": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8da7fbc5755d44778e0e25a89c7429d2",
        "IPY_MODEL_68c2837357514fd5bc8386121af1430b",
        "IPY_MODEL_3eb444c87ffb42b69fac4b1694a10843"
       ],
       "layout": "IPY_MODEL_642f553810d7402fb9adab4eeeb42733",
       "tabbable": null,
       "tooltip": null
      }
     },
     "642f553810d7402fb9adab4eeeb42733": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "68c2837357514fd5bc8386121af1430b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b4e46c45a31d4753ba394503644be1af",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f924d4539a9f4e099ec167b3cb2f296a",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "6cd2f2332ca94f2a86bceedc5c957ce2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7e41fd18170d40abbcaaa8a034e26638": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_89428be88c5b492fb58856dcf2f94b14",
       "placeholder": "​",
       "style": "IPY_MODEL_24be7c7e28f3454e95762c889a575126",
       "tabbable": null,
       "tooltip": null,
       "value": " 200/200 [00:00&lt;00:00, 1101.78 examples/s]"
      }
     },
     "8109f5ea9d5940df9a9b45c47dc70fa3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4261e71c289b41a79b0bf5788b089440",
       "placeholder": "​",
       "style": "IPY_MODEL_aebcfe04209247ecb11294b07768bb8a",
       "tabbable": null,
       "tooltip": null,
       "value": " 2/2 [00:02&lt;00:00,  1.01it/s]"
      }
     },
     "89428be88c5b492fb58856dcf2f94b14": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8da7fbc5755d44778e0e25a89c7429d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ab6a54b7644645a996f43070ef2595bf",
       "placeholder": "​",
       "style": "IPY_MODEL_1e5af73395424db8bfb4fe5502481e0f",
       "tabbable": null,
       "tooltip": null,
       "value": "Computing checksums: 100%"
      }
     },
     "982ee5642a634d698866275620db3c89": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3382da1816c543e98d405cb7d4c3c1eb",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b5fe32714ab3405aa0b0d3003dc94634",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "98452dd6bf8945a68c70c8ac57ffc371": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9c6481d7cfb6423d911b75a128c42e0e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9d489dc456f2456982c06a3be18bf948": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a077a3ccb1f546449568a1279b43d638": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ab6a54b7644645a996f43070ef2595bf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ab7b33fd096a4c56aaeb31eac37a2ca6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b0aae8ec3a1c4d8284c41cc097c89e69",
       "placeholder": "​",
       "style": "IPY_MODEL_63d43ae1b85c4a04b2f7aa28bcd34a1e",
       "tabbable": null,
       "tooltip": null,
       "value": " 2/2 [00:02&lt;00:00,  1.09s/it]"
      }
     },
     "aebcfe04209247ecb11294b07768bb8a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b005f0589ff544c2a2d41109d39a9272": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_cc650e3ade9e42b2b60c58eb7652ba03",
       "placeholder": "​",
       "style": "IPY_MODEL_9d489dc456f2456982c06a3be18bf948",
       "tabbable": null,
       "tooltip": null,
       "value": "Generating train split: "
      }
     },
     "b0aae8ec3a1c4d8284c41cc097c89e69": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b4b0b6863199443382515e6c070c7386": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b4e46c45a31d4753ba394503644be1af": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b4e53b5db4f34d49a22e4d09d373d326": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b5fe32714ab3405aa0b0d3003dc94634": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b723504eb3024d6baabe1d4896d71d08": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_066cbeeb05fc43deb600a86ed0125fda",
       "max": 200.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_db780a834c76419da5eef29b7639363d",
       "tabbable": null,
       "tooltip": null,
       "value": 200.0
      }
     },
     "b7d544b7c3ae4bb6941deddd0799e946": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_cbd5acc48600464aaa64be81cd467f3e",
        "IPY_MODEL_42ea922fac7b4269bbce334d88fbe12b",
        "IPY_MODEL_8109f5ea9d5940df9a9b45c47dc70fa3"
       ],
       "layout": "IPY_MODEL_48a45a62e9f144a3b98579d281bf7c43",
       "tabbable": null,
       "tooltip": null
      }
     },
     "b97c3c2a7c9748ef92d71079f8b0430b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c7f175d5d02044ca9e4c81fa8bba6840": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c914986343574503bb5d2a3534ae629c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cbd5acc48600464aaa64be81cd467f3e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b4b0b6863199443382515e6c070c7386",
       "placeholder": "​",
       "style": "IPY_MODEL_1cce82aad91d40eca9cba30bb3422488",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "cc650e3ade9e42b2b60c58eb7652ba03": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ce9fc1aee02845d9883177f0a8ff426d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0814f3a0b9534d95a4ded6ee9f5b2d83",
        "IPY_MODEL_1c98a70f399c4c1aa1344adc68948aac",
        "IPY_MODEL_ab7b33fd096a4c56aaeb31eac37a2ca6"
       ],
       "layout": "IPY_MODEL_5d23118534034678a91c4789d62eefb5",
       "tabbable": null,
       "tooltip": null
      }
     },
     "db780a834c76419da5eef29b7639363d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f924d4539a9f4e099ec167b3cb2f296a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
